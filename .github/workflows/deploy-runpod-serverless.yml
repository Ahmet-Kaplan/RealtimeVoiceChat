name: Deploy RunPod Serverless Endpoints

on:
  push:
    paths:
      - '.github/workflows/deploy-runpod-serverless.yml'
    branches: [main]
  workflow_run:
    workflows: ["Build and Push Whisper Worker", "Build and Push TTS Worker", "Build and Push LLM Worker"]
    types:
      - completed
    branches: [main]
  # Allow manual triggering
  workflow_dispatch:
    inputs:
      whisper_image:
        description: 'Whisper Docker image to deploy'
        required: true
        type: string
      tts_image:
        description: 'TTS Docker image to deploy'
        required: true
        type: string
      llm_image:
        description: 'LLM Docker image to deploy'
        required: true
        type: string

jobs:
  deploy:
    runs-on: ubuntu-latest
    # Only run if the triggering workflow was successful (for workflow_run events)
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' || github.event_name == 'push' }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install RunPod CLI and dependencies
        run: |
          pip install runpod requests

      - name: Deploy Endpoints to RunPod
        run: |
          # Get the Docker image names from the workflow inputs or build workflows
          WHISPER_IMAGE="${{ github.event.inputs.whisper_image || format('{0}/realtime-voice-whisper:latest', secrets.DOCKERHUB_USERNAME) }}"
          TTS_IMAGE="${{ github.event.inputs.tts_image || format('{0}/realtime-voice-tts:latest', secrets.DOCKERHUB_USERNAME) }}"
          LLM_IMAGE="${{ github.event.inputs.llm_image || format('{0}/realtime-voice-llm:latest', secrets.DOCKERHUB_USERNAME) }}"
          
          # Deploy to RunPod Serverless
          ENDPOINT_RESPONSE=$(python3 -c '
          import runpod
          import json
          import os
          
          # Initialize RunPod with API key
          runpod.api_key = os.environ["RUNPOD_API_KEY"]
          
          # Check existing endpoints
          endpoints = runpod.get_endpoints()
          
          # Get templates using GraphQL query
          templates_query = """
          query {
            myself {
              templates {
                id
                name
                imageName
                isServerless
              }
            }
          }
          """
          templates_response = runpod.query(templates_query)
          templates = templates_response.get("data", {}).get("myself", {}).get("templates", [])
          
          endpoint_ids = {
              "whisper": None,
              "tts": None,
              "llm": None
          }
          
          template_ids = {
              "whisper": None,
              "tts": None,
              "llm": None
          }
          
          # Find existing endpoints
          for endpoint in endpoints:
              name = endpoint.get("name")
              if name == "whisper-worker":
                  endpoint_ids["whisper"] = endpoint.get("id")
              elif name == "tts-worker":
                  endpoint_ids["tts"] = endpoint.get("id")
              elif name == "llm-worker":
                  endpoint_ids["llm"] = endpoint.get("id")
          
          # Find existing templates
          for template in templates:
              name = template.get("name")
              if name == "whisper-worker-template":
                  template_ids["whisper"] = template.get("id")
              elif name == "tts-worker-template":
                  template_ids["tts"] = template.get("id")
              elif name == "llm-worker-template":
                  template_ids["llm"] = template.get("id")
          
          responses = {}
          
          # Get or create Whisper template
          if not template_ids["whisper"]:
              whisper_template = runpod.create_template(
                  name="whisper-worker-template",
                  image_name="$WHISPER_IMAGE",
                  is_serverless=True
              )
              template_ids["whisper"] = whisper_template["id"]
          
          # Update or create Whisper endpoint
          if endpoint_ids["whisper"]:
              responses["whisper"] = runpod.update_endpoint(
                  endpoint_id=endpoint_ids["whisper"],
                  template_id=template_ids["whisper"],
                  gpu_ids=["NVIDIA RTX A5000"],
                  name="whisper-worker",
                  active=True
              )
          else:
              responses["whisper"] = runpod.create_endpoint(
                  name="whisper-worker",
                  template_id=template_ids["whisper"],
                  gpu_ids=["NVIDIA RTX A5000"],
                  workers_max=1
              )
          
          # Get or create TTS template
          if not template_ids["tts"]:
              tts_template = runpod.create_template(
                  name="tts-worker-template",
                  image_name="$TTS_IMAGE",
                  is_serverless=True
              )
              template_ids["tts"] = tts_template["id"]
          
          # Update or create TTS endpoint
          if endpoint_ids["tts"]:
              responses["tts"] = runpod.update_endpoint(
                  endpoint_id=endpoint_ids["tts"],
                  template_id=template_ids["tts"],
                  gpu_ids=["NVIDIA RTX A5000"],
                  name="tts-worker",
                  active=True
              )
          else:
              responses["tts"] = runpod.create_endpoint(
                  name="tts-worker",
                  template_id=template_ids["tts"],
                  gpu_ids=["NVIDIA RTX A5000"],
                  workers_max=1
              )
          
          # Get or create LLM template
          if not template_ids["llm"]:
              llm_template = runpod.create_template(
                  name="llm-worker-template",
                  image_name="$LLM_IMAGE",
                  is_serverless=True
              )
              template_ids["llm"] = llm_template["id"]
          
          # Update or create LLM endpoint
          if endpoint_ids["llm"]:
              responses["llm"] = runpod.update_endpoint(
                  endpoint_id=endpoint_ids["llm"],
                  template_id=template_ids["llm"],
                  gpu_ids=["NVIDIA RTX A5000"],
                  name="llm-worker",
                  active=True
              )
          else:
              responses["llm"] = runpod.create_endpoint(
                  name="llm-worker",
                  template_id=template_ids["llm"],
                  gpu_ids=["NVIDIA RTX A5000"],
                  workers_max=1
              )
          
          print(json.dumps(responses))
          ')
          
          # Extract endpoint IDs and save them
          WHISPER_ENDPOINT_ID=$(echo $ENDPOINT_RESPONSE | jq -r '.whisper.id')
          TTS_ENDPOINT_ID=$(echo $ENDPOINT_RESPONSE | jq -r '.tts.id')
          LLM_ENDPOINT_ID=$(echo $ENDPOINT_RESPONSE | jq -r '.llm.id')
          
          echo "Whisper endpoint deployed: $WHISPER_ENDPOINT_ID"
          echo "TTS endpoint deployed: $TTS_ENDPOINT_ID"
          echo "LLM endpoint deployed: $LLM_ENDPOINT_ID"
          
          # Save endpoint IDs to GitHub environment
          echo "WHISPER_ENDPOINT_ID=$WHISPER_ENDPOINT_ID" >> $GITHUB_ENV
          echo "TTS_ENDPOINT_ID=$TTS_ENDPOINT_ID" >> $GITHUB_ENV
          echo "LLM_ENDPOINT_ID=$LLM_ENDPOINT_ID" >> $GITHUB_ENV
        env:
          RUNPOD_API_KEY: ${{ secrets.RUNPOD_API_KEY }}

      - name: Summary
        run: |
          echo "✅ Deployment Summary:"
          echo "All RunPod Serverless endpoints have been deployed successfully."
          echo "Your Next.js application will automatically fetch the latest endpoint IDs at runtime."
          echo ""
          echo "⚠️ Important:"
          echo "1. Make sure to set RUNPOD_API_KEY in your backend environment variables"

      # Optional: Deploy the client app to Vercel
      # - name: Deploy to Vercel
      #   uses: amondnet/vercel-action@v20
      #   with:
      #     vercel-token: ${{ secrets.VERCEL_TOKEN }}
      #     vercel-org-id: ${{ secrets.VERCEL_ORG_ID }}
      #     vercel-project-id: ${{ secrets.VERCEL_PROJECT_ID }}
      #     working-directory: client-app 